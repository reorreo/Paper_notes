# ITE-MTA


## Title List

1. [Twin Contrastive Learning With Noisy Labels (CVPR2023):TCL]
2. [Longremix: Robust learning with high confidence samples in a noisy label environment (2023)):LongReMix]
3. [A Simple Framework for Contrastive Learning of Visual Representations (PMLR2023):SimCLR]
4. [Momentum Contrast for Unsupervised Visual Representation Learning(CVPR2020):MoCo]
5. [Exploring Simple Siamese Representation Learning(CVPR2021):SimSiam]
6. [Representation Learning with Contrastive Predictive Coding(2018)]
7. [Bootstrap your own latent-a new approach to self-supervised learning(2020):BYOL]
8. [Learning From Noisy Data With Robust Representation Learning(ICCV2021):RRL]
9. [Multi-Objective Interpolation Training for Robustness To Label Noise(CVPR2021):MOIT]
10. [Dividemix: Learning with noisy labels as semi-supervised learning(2020):DivideMix]
11. [mixup: Beyond empirical risk minimization(2017):MixUp]
12. [Probabilistic End-To-End Noise Correction for Learning With Noisy Labels(CVPR2019)]
13. [Unsupervised Label Noise Modeling and Loss Correction(PMLR2019)]
14. [Early-learning regularization prevents memorization of noisy labels(NIPS2020):ELR]
15. [Selective-Supervised Contrastive Learning With Noisy Labels(CVPR2022):Sel-CL]
16. [On Learning Contrastive Representations for Learning With Noisy Labels(CVPR2022):CTRR]
17. [Probabilistic End-To-End Noise Correction for Learning With Noisy Labels(CVPR2019)]
18. [Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates(PMLR2020)]
19. [Robust Training under Label Noise by Over-parameterization(PMLR2022)]
20. [A Closer Look at Memorization in Deep Networks(PMLR2017)]
21. [Augmentation Strategies for Learning With Noisy Labels(CVPR2021)]
---


### Twin Contrastive Learning With Noisy Labels (CVPR2023):TCL
[[Paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Twin_Contrastive_Learning_With_Noisy_Labels_CVPR_2023_paper.pdf)
[[Code]](https://github.com/Hzzone/TCL)
[[bibtex]](https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Twin_Contrastive_Learning_With_Noisy_Labels_CVPR_2023_paper.html)
<details><summary>summary</summary><div>

</div></details> 

### Longremix: Robust learning with high confidence samples in a noisy label environment (2023):LongReMix
[[Paper]](https://www.sciencedirect.com/science/article/pii/S0031320322004939/pdfft?md5=21004c446dccd13a5cd59f6901a41607&pid=1-s2.0-S0031320322004939-main.pdf)
[[Code]](https://github.com/filipe-research/LongReMix)
[[bibtex]](https://www.sciencedirect.com/science/article/pii/S0031320322004939)
<details><summary>summary</summary><div>

</div></details> 

##  対照学習
### A Simple Framework for Contrastive Learning of Visual Representations (PMLR2023):SimCLR
[[Paper]](http://proceedings.mlr.press/v119/chen20j/chen20j.pdf)
[[Code]](https://github.com/google-research/simclr)
[[bibtex]](https://proceedings.mlr.press/v119/chen20j.html)
<details><summary>summary</summary><div>
  
</div></details> 
- Keywords : `Contrastive Learning`


### Momentum Contrast for Unsupervised Visual Representation Learning(CVPR2020):MoCo
[[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf)
[]([[Code]])
[[bibtex]](https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.htmll)
<details><summary>summary</summary><div>
  
</div></details> 
- Keywords : `Contrastive Learning`

### Exploring Simple Siamese Representation Learning(CVPR2021):SimSiam
[[Paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.pdf)
[[Code]](https://github.com/facebookresearch/simsiam)
[[bibtex]](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.html)
<details><summary>summary</summary><div>
  
</div></details> 
- Keywords : `Contrastive Learning`

### Representation Learning with Contrastive Predictive Coding(2018):BYOL
[[Paper]](https://arxiv.org/pdf/1807.03748)
[[bibtex]](https://arxiv.org/abs/1807.03748)
<details><summary>summary</summary><div>
  TCL内でInfoNCEの説明に使われている．
</div></details> 
- Keywords : `Contrastive Learning`


### Bootstrap your own latent-a new approach to self-supervised learning(2020)
[[Paper]](https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf)
<details><summary>summary</summary><div>

</div></details> 
- Keywords : `Contrastive Learning`

### Learning From Noisy Data With Robust Representation Learning(ICCV2021):RRL
[[Paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Learning_From_Noisy_Data_With_Robust_Representation_Learning_ICCV_2021_paper.pdf)
[[bibtex]](https://openaccess.thecvf.com/content/ICCV2021/html/Li_Learning_From_Noisy_Data_With_Robust_Representation_Learning_ICCV_2021_paper.html)
<details><summary>summary</summary><div>
InfoNCEの損失をMixUpしている?．今後の研究でしたいことと似ているため読む必要がある．
対照学習を用いたノイジーラベルに対する手法の例として挙げられていた．
</div></details> 
- Keywords : `Contrastive Learning` 'MixUp'

### Multi-Objective Interpolation Training for Robustness To Label Noise(CVPR2021):MOIT
[[Paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Ortego_Multi-Objective_Interpolation_Training_for_Robustness_To_Label_Noise_CVPR_2021_paper.pdf)
[[bibtex]](https://openaccess.thecvf.com/content/CVPR2021/html/Ortego_Multi-Objective_Interpolation_Training_for_Robustness_To_Label_Noise_CVPR_2021_paper.html)
<details><summary>summary</summary><div>
特徴量空間でMixUpしている?．今後の研究でしたいことと似ているため読む必要がある．
対照学習を用いたノイジーラベルに対する手法の例として挙げられていた．
</div></details> 
- Keywords : `Contrastive Learning` 'MixUp'


### Dividemix: Learning with noisy labels as semi-supervised learning(2020):DivideMix
[[Paper]](https://arxiv.org/pdf/2002.07394)
[[bibtex]](https://arxiv.org/abs/2002.07394)
<details><summary>summary</summary><div>
co-
</div></details> 
- Keywords : `` 'MixUp'

## TCL内の比較してる手法

### mixup: Beyond empirical risk minimization(2017):MixUp
[[Paper]](https://arxiv.org/pdf/1710.09412)
[[bibtex]](https://arxiv.org/abs/1710.09412)
<details><summary>summary</summary><div>

</div></details> 
- Keywords : `` 'MixUp'

### Probabilistic End-To-End Noise Correction for Learning With Noisy Labels(CVPR2019)
[[Paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Yi_Probabilistic_End-To-End_Noise_Correction_for_Learning_With_Noisy_Labels_CVPR_2019_paper.pdf)
[[bibtex]](https://openaccess.thecvf.com/content_CVPR_2019/html/Yi_Probabilistic_End-To-End_Noise_Correction_for_Learning_With_Noisy_Labels_CVPR_2019_paper.html)
<details><summary>summary</summary><div>
  
</div></details> 

### Unsupervised Label Noise Modeling and Loss Correction(PMLR2019)
[[Paper]](http://proceedings.mlr.press/v97/arazo19a/arazo19a.pdf)
[[bibtex]](https://proceedings.mlr.press/v97/arazo19a.html)
<details><summary>summary</summary><div>

</div></details> 

### Early-learning regularization prevents memorization of noisy labels(NIPS2020):ELR
[[Paper]](http://proceedings.mlr.press/v97/arazo19a/arazo19a.pdf)
[[bibtex]](https://proceedings.mlr.press/v97/arazo19a.html)
<details><summary>summary</summary><div>

</div></details> 

### Selective-Supervised Contrastive Learning With Noisy Labels(CVPR2022):Sel-CL
[[Paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Selective-Supervised_Contrastive_Learning_With_Noisy_Labels_CVPR_2022_paper.pdf)
[[code]](https://github.com/ShikunLi/Sel-CL)
[[bibtex]](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Selective-Supervised_Contrastive_Learning_With_Noisy_Labels_CVPR_2022_paper.html)
<details><summary>summary</summary><div>

</div></details> 
- Keywords : `Contrastive Learning` '提案手法を組み込む候補' 


## RankMatch内の比較してる手法
### On Learning Contrastive Representations for Learning With Noisy Labels(CVPR2022):CTRR
[[Paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Yi_On_Learning_Contrastive_Representations_for_Learning_With_Noisy_Labels_CVPR_2022_paper.pdf)
[[bibtex]](https://openaccess.thecvf.com/content/CVPR2022/html/Yi_On_Learning_Contrastive_Representations_for_Learning_With_Noisy_Labels_CVPR_2022_paper.html)
<details><summary>summary</summary><div>

</div></details> 
- Keywords : `Contrastive Learning`

### Probabilistic End-To-End Noise Correction for Learning With Noisy Labels(CVPR2019)
[[Paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Yi_Probabilistic_End-To-End_Noise_Correction_for_Learning_With_Noisy_Labels_CVPR_2019_paper.pdf)
[[bibtex]](https://openaccess.thecvf.com/content_CVPR_2019/html/Yi_Probabilistic_End-To-End_Noise_Correction_for_Learning_With_Noisy_Labels_CVPR_2019_paper.htmll)
<details><summary>summary</summary><div>
ノイズの多いラベルを修正するために、別の一連の研究では、ノイズの多いラベルをネットワーク予測で置き換える自己学習アーキテクチャを提案している
</div></details>
- Keywords : `Contrastive Learning`

### Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates(PMLR2020)
[[Paper]](http://proceedings.mlr.press/v119/liu20e/liu20e.pdf)
[[bibtex]](https://proceedings.mlr.press/v119/liu20e.html)
<details><summary>summary</summary><div>
punishment regularization(罰正規化)
</div></details> 

### Robust Training under Label Noise by Over-parameterization(PMLR2022)
[[Paper]](https://proceedings.mlr.press/v162/liu22w/liu22w.pdf)
[[bibtex]](https://proceedings.mlr.press/v162/liu22w.html)
<details><summary>summary</summary><div>
over parameterized term(過剰パラメータ化項)
</div></details> 

### A Closer Look at Memorization in Deep Networks(PMLR2017)
[[Paper]](http://proceedings.mlr.press/v70/arpit17a/arpit17a.pdf)
[[bibtex]](https://proceedings.mlr.press/v70/arpit17a.html)
<details><summary>summary</summary><div>
Memorization Efectについか書かれている論文
</div></details> 
- Keywords : `Memorization Efect`

### Augmentation Strategies for Learning With Noisy Labels(CVPR2021)
[[Paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Nishi_Augmentation_Strategies_for_Learning_With_Noisy_Labels_CVPR_2021_paper.pdf)
[[bibtex]](https://openaccess.thecvf.com/content/CVPR2021/html/Nishi_Augmentation_Strategies_for_Learning_With_Noisy_Labels_CVPR_2021_paper.html)
<details><summary>summary</summary><div>
損失からGMMを用いてラベルがクリーンである確率を計算している手法．
論文で引くときはDivideMixも一緒にの場所に引用する予定．
</div></details> 

## データセット系
